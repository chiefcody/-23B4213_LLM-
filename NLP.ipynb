{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GY8rvdwlupY3",
        "outputId": "c0d9627a-e09d-4314-9731-5ff65fdf53fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: nltk in c:\\users\\aditya\\appdata\\roaming\\python\\python312\\site-packages (3.8.1)\n",
            "Requirement already satisfied: click in c:\\users\\aditya\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\users\\aditya\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\aditya\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in c:\\users\\aditya\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\aditya\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 24.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wijXXvsWF7w7",
        "outputId": "06034176-6cf1-411a-c0a6-36ba40840122"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Aditya\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7Y_hgxozE6kA"
      },
      "outputs": [],
      "source": [
        "corpus = \"Howdy! Hello my name is Aditya Gavali. I'm from Pune is in Maha.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnz5C1uiFXBJ",
        "outputId": "7f94f1ea-a8ab-4b97-99e1-321449459ee7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Howdy! Hello my name is Aditya Gavali. I'm from Pune is in Maha.\n"
          ]
        }
      ],
      "source": [
        "print(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2H4Ot_2jFc6e"
      },
      "outputs": [],
      "source": [
        "# Tokenization\n",
        "# Converts Paragraph ---> sentences\n",
        "from nltk.tokenize import sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzGpIBJmFubW",
        "outputId": "7d980779-0509-4dc2-87b8-542a0bd90af0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Howdy!', 'Hello my name is Aditya Gavali.', \"I'm from Pune is in Maha.\"]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sent_tokenize(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PQUMzC20GCNz"
      },
      "outputs": [],
      "source": [
        "documents = sent_tokenize(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0St1ra63GM3r",
        "outputId": "67ac4ca3-011a-421f-8837-22796ce6a5d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djLQ8aVHGPx8",
        "outputId": "c8abeac7-d950-4a71-b5e0-1d657119388f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Howdy!\n",
            "Hello my name is Aditya Gavali.\n",
            "I'm from Pune is in Maha.\n"
          ]
        }
      ],
      "source": [
        "for sentence in documents:\n",
        "  print(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ddtcr3VLGZ9s"
      },
      "outputs": [],
      "source": [
        "# Tokenization\n",
        "# sentences to words\n",
        "# paragraphs to words\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6kN5NwiGtbm",
        "outputId": "df5a0916-1550-43be-e1eb-2c1cef1f9928"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Howdy',\n",
              " '!',\n",
              " 'Hello',\n",
              " 'my',\n",
              " 'name',\n",
              " 'is',\n",
              " 'Aditya',\n",
              " 'Gavali',\n",
              " '.',\n",
              " 'I',\n",
              " \"'m\",\n",
              " 'from',\n",
              " 'Pune',\n",
              " 'is',\n",
              " 'in',\n",
              " 'Maha',\n",
              " '.']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_tokenize(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UXzZ5_iG0Hz",
        "outputId": "80a4636c-9a91-45a2-8aa1-11385fd0e32b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Howdy', '!']\n",
            "['Hello', 'my', 'name', 'is', 'Aditya', 'Gavali', '.']\n",
            "['I', \"'m\", 'from', 'Pune', 'is', 'in', 'Maha', '.']\n"
          ]
        }
      ],
      "source": [
        "for sentence in documents:\n",
        "  print(word_tokenize(sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_lTdQiCG-RI",
        "outputId": "3d0c513b-420d-45f7-fe3e-1d0413a95a28"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Howdy',\n",
              " '!',\n",
              " 'Hello',\n",
              " 'my',\n",
              " 'name',\n",
              " 'is',\n",
              " 'Aditya',\n",
              " 'Gavali',\n",
              " '.',\n",
              " 'I',\n",
              " \"'\",\n",
              " 'm',\n",
              " 'from',\n",
              " 'Pune',\n",
              " 'is',\n",
              " 'in',\n",
              " 'Maha',\n",
              " '.']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.tokenize import wordpunct_tokenize\n",
        "wordpunct_tokenize(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qI7enkQ1HLUm"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer = TreebankWordTokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzY-nesOHoGe",
        "outputId": "fef7204a-4c31-4870-d839-5ff4dbdaa7d9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Howdy',\n",
              " '!',\n",
              " 'Hello',\n",
              " 'my',\n",
              " 'name',\n",
              " 'is',\n",
              " 'Aditya',\n",
              " 'Gavali.',\n",
              " 'I',\n",
              " \"'m\",\n",
              " 'from',\n",
              " 'Pune',\n",
              " 'is',\n",
              " 'in',\n",
              " 'Maha',\n",
              " '.']"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.tokenize(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "voU_A1EZHuVi"
      },
      "outputs": [],
      "source": [
        "\n",
        "words = [\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalized\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Porter Stemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "MyPd5hVNKBZS"
      },
      "outputs": [],
      "source": [
        "\n",
        "from nltk.stem import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "v1_W2DD-KQoP"
      },
      "outputs": [],
      "source": [
        "stemming = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4sF-DdDKToE",
        "outputId": "dcbe0e25-5df8-4aac-e785-eabe494b770f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eating--->eat\n",
            "eats--->eat\n",
            "eaten--->eaten\n",
            "writing--->write\n",
            "writes--->write\n",
            "programming--->program\n",
            "programs--->program\n",
            "history--->histori\n",
            "finally--->final\n",
            "finalized--->final\n"
          ]
        }
      ],
      "source": [
        "for word in words:\n",
        "  print(word+\"--->\"+stemming.stem(word))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "History --> Histori "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#RegexpStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "SwgV1RwUKvHY"
      },
      "outputs": [],
      "source": [
        "# RegexpStemmer class\n",
        "from nltk.stem import RegexpStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ucrFdFIZMDRp"
      },
      "outputs": [],
      "source": [
        "reg_stemmer = RegexpStemmer('ing$|s$|e$|able$' , min = 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "YSH6R_jDNTm-",
        "outputId": "5363d9cd-f19c-4327-f542-6d7f0cea6da6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'eat'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reg_stemmer.stem('eating')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "wPOhYef2Ng7j",
        "outputId": "d9e7d489-d233-4b81-8392-6be8b8da4e29"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'ingeat'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reg_stemmer.stem('ingeating')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#Snowball Stemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "s0p_m4CaN9CT"
      },
      "outputs": [],
      "source": [
        "\n",
        "from nltk.stem import SnowballStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "hmtupmODPAtw"
      },
      "outputs": [],
      "source": [
        "snowball = SnowballStemmer('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoxWHUHTPOSi",
        "outputId": "45986382-7ca3-4560-8fbe-1082c7ab9766"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eating---->eat\n",
            "eats---->eat\n",
            "eaten---->eaten\n",
            "writing---->write\n",
            "writes---->write\n",
            "programming---->program\n",
            "programs---->program\n",
            "history---->histori\n",
            "finally---->final\n",
            "finalized---->final\n"
          ]
        }
      ],
      "source": [
        "for word in words:\n",
        "  print(word+\"---->\"+snowball.stem(word))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Why Snowball stemmer is Better "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2RX0FIqQ6in",
        "outputId": "e5010884-77a7-4de2-d885-4084030ab7a4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('fairli', 'sportingli')"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stemming.stem(\"fairly\"), stemming.stem(\"sportingly\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zt6qMw5BRLQL",
        "outputId": "b2a55279-3f6d-4b86-dbcd-fbf7d0e8cfee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('fair', 'sport')"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "snowball.stem(\"fairly\"), snowball.stem(\"sportingly\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Some exceptions like This ( histori)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'goe'"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "snowballstemmer.stem(\"goes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'goe'"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stemming.stem(\"goes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Wordnet Lemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "uHvKNPVjRiNm"
      },
      "outputs": [],
      "source": [
        "\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Z-lGYIrPVFdm"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYIjKwmRVdPd",
        "outputId": "4cd329e2-31d6-414c-d417-4a3edbdf642b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Aditya\\AppData\\Roaming\\nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "M4LvpO__VP9L",
        "outputId": "59c529c7-6b44-4444-afbb-c6f390da3ef5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'go'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "POS - Noun - n\n",
        "verb - v\n",
        "adjective - a\n",
        "adverb -r\n",
        "\"\"\"\n",
        "lemmatizer.lemmatize(\"going\", pos = 'v')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiSQO-JNVjuo",
        "outputId": "74af0ebe-6903-4791-f430-8e7cbd0074be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eating---->eat\n",
            "eats---->eat\n",
            "eaten---->eat\n",
            "writing---->write\n",
            "writes---->write\n",
            "programming---->program\n",
            "programs---->program\n",
            "history---->history\n",
            "finally---->finally\n",
            "finalized---->finalize\n"
          ]
        }
      ],
      "source": [
        "for word in words:\n",
        "  print(word+\"---->\"+lemmatizer.lemmatize(word,pos = 'v'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('fairly', 'sportingly')"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lemmatizer.lemmatize(\"fairly\", pos = 'v'),lemmatizer.lemmatize(\"sportingly\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Stopwords with NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "APAZhzOCXeVl"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "## Speech Of DR APJ Abdul Kalam\n",
        "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over\n",
        "               the world have come and invaded us, captured our lands, conquered our minds.\n",
        "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
        "               the French, the Dutch, all of them came and looted us, took over what was ours.\n",
        "               Yet we have not done this to any other nation. We have not conquered anyone.\n",
        "               We have not grabbed their land, their culture,\n",
        "               their history and tried to enforce our way of life on them.\n",
        "               Why? Because we respect the freedom of others.That is why my\n",
        "               first vision is that of freedom. I believe that India got its first vision of\n",
        "               this in 1857, when we started the War of Independence. It is this freedom that\n",
        "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
        "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
        "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
        "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
        "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
        "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
        "               I have a third vision. India must stand up to the world. Because I believe that unless India\n",
        "               stands up to the world, no one will respect us. Only strength respects strength. We must be\n",
        "               strong not only as a military power but also as an economic power. Both must go hand-in-hand.\n",
        "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of\n",
        "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
        "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.\n",
        "               I see four milestones in my career\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "m0WaPplm6asd"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voFIkw-R6_Y2",
        "outputId": "b096e30b-2442-420c-ed3c-40a2cc86ea37"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Aditya\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Spq2i2zH7DhP",
        "outputId": "ad0c4325-0653-438e-8a8f-098abfb4dacb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ytom_mv_7d8y"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "stemmer=PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['I have three visions for India.',\n",
              " 'In 3000 years of our history, people from all over\\n               the world have come and invaded us, captured our lands, conquered our minds.',\n",
              " 'From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\\n               the French, the Dutch, all of them came and looted us, took over what was ours.',\n",
              " 'Yet we have not done this to any other nation.',\n",
              " 'We have not conquered anyone.',\n",
              " 'We have not grabbed their land, their culture,\\n               their history and tried to enforce our way of life on them.',\n",
              " 'Why?',\n",
              " 'Because we respect the freedom of others.That is why my\\n               first vision is that of freedom.',\n",
              " 'I believe that India got its first vision of\\n               this in 1857, when we started the War of Independence.',\n",
              " 'It is this freedom that\\n               we must protect and nurture and build on.',\n",
              " 'If we are not free, no one will respect us.',\n",
              " 'My second vision for India’s development.',\n",
              " 'For fifty years we have been a developing nation.',\n",
              " 'It is time we see ourselves as a developed nation.',\n",
              " 'We are among the top 5 nations of the world\\n               in terms of GDP.',\n",
              " 'We have a 10 percent growth rate in most areas.',\n",
              " 'Our poverty levels are falling.',\n",
              " 'Our achievements are being globally recognised today.',\n",
              " 'Yet we lack the self-confidence to\\n               see ourselves as a developed nation, self-reliant and self-assured.',\n",
              " 'Isn’t this incorrect?',\n",
              " 'I have a third vision.',\n",
              " 'India must stand up to the world.',\n",
              " 'Because I believe that unless India\\n               stands up to the world, no one will respect us.',\n",
              " 'Only strength respects strength.',\n",
              " 'We must be\\n               strong not only as a military power but also as an economic power.',\n",
              " 'Both must go hand-in-hand.',\n",
              " 'My good fortune was to have worked with three great minds.',\n",
              " 'Dr. Vikram Sarabhai of the Dept.',\n",
              " 'of\\n               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.',\n",
              " 'I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.',\n",
              " 'I see four milestones in my career']"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.sent_tokenize(paragraph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "0ovXLABU76-4"
      },
      "outputs": [],
      "source": [
        "sentences=nltk.sent_tokenize(paragraph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbufuKvE8GDe",
        "outputId": "ec5bbb25-f4ce-4fe8-e63c-c85e97c5ccdc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apply Stopwords and Filter and then apply Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "zYIkI-ko8Lmq"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "    words=nltk.word_tokenize(sentences[i])\n",
        "    words=[stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "    sentences[i]=' '.join(words)# converting all the words into sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRIeORee8QPm",
        "outputId": "868aff2f-ebeb-45c6-c77a-705a9feb5749"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['i three vision india .',\n",
              " 'in 3000 year histori , peopl world come invad us , captur land , conquer mind .',\n",
              " 'from alexand onward , greek , turk , mogul , portugues , british , french , dutch , came loot us , took .',\n",
              " 'yet done nation .',\n",
              " 'we conquer anyon .',\n",
              " 'we grab land , cultur , histori tri enforc way life .',\n",
              " 'whi ?',\n",
              " 'becaus respect freedom others.that first vision freedom .',\n",
              " 'i believ india got first vision 1857 , start war independ .',\n",
              " 'it freedom must protect nurtur build .',\n",
              " 'if free , one respect us .',\n",
              " 'my second vision india ’ develop .',\n",
              " 'for fifti year develop nation .',\n",
              " 'it time see develop nation .',\n",
              " 'we among top 5 nation world term gdp .',\n",
              " 'we 10 percent growth rate area .',\n",
              " 'our poverti level fall .',\n",
              " 'our achiev global recognis today .',\n",
              " 'yet lack self-confid see develop nation , self-reli self-assur .',\n",
              " 'isn ’ incorrect ?',\n",
              " 'i third vision .',\n",
              " 'india must stand world .',\n",
              " 'becaus i believ unless india stand world , one respect us .',\n",
              " 'onli strength respect strength .',\n",
              " 'we must strong militari power also econom power .',\n",
              " 'both must go hand-in-hand .',\n",
              " 'my good fortun work three great mind .',\n",
              " 'dr. vikram sarabhai dept .',\n",
              " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .',\n",
              " 'i lucki work three close consid great opportun life .',\n",
              " 'i see four mileston career']"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "WDxTA6qX90b1"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import SnowballStemmer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "snowballstemmer=SnowballStemmer('english')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apply Stopwords and Filter and then apply Snowball Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "66JbFY2N95Us"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "    words=nltk.word_tokenize(sentences[i])\n",
        "    words=[snowballstemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "    sentences[i]=' '.join(words)# converting all the list of words into sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9TzGPQp99Jl",
        "outputId": "379a2eea-1591-4041-c313-a94f37a31832"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['three vision india .',\n",
              " '3000 year histori , peopl world come invad us , captur land , conquer mind .',\n",
              " 'alexand onward , greek , turk , mogul , portugu , british , french , dutch , came loot us , took .',\n",
              " 'yet done nation .',\n",
              " 'conquer anyon .',\n",
              " 'grab land , cultur , histori tri enforc way life .',\n",
              " 'whi ?',\n",
              " 'becaus respect freedom others.that first vision freedom .',\n",
              " 'believ india got first vision 1857 , start war independ .',\n",
              " 'freedom must protect nurtur build .',\n",
              " 'free , one respect us .',\n",
              " 'second vision india ’ develop .',\n",
              " 'fifti year develop nation .',\n",
              " 'time see develop nation .',\n",
              " 'among top 5 nation world term gdp .',\n",
              " '10 percent growth rate area .',\n",
              " 'poverti level fall .',\n",
              " 'achiev global recogni today .',\n",
              " 'yet lack self-confid see develop nation , self-r self-assur .',\n",
              " '’ incorrect ?',\n",
              " 'third vision .',\n",
              " 'india must stand world .',\n",
              " 'becaus believ unless india stand world , one respect us .',\n",
              " 'on strength respect strength .',\n",
              " 'must strong militari power also econom power .',\n",
              " 'must go hand-in-hand .',\n",
              " 'good fortun work three great mind .',\n",
              " 'dr. vikram sarabhai dept .',\n",
              " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .',\n",
              " 'lucki work three close consid great opportun life .',\n",
              " 'see four mileston career']"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "RJOFbY5E-A7O"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer=WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "W9Yv40S4-Ett"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "    #sentences[i]=sentences[i].lower()\n",
        "    words=nltk.word_tokenize(sentences[i])\n",
        "    words=[lemmatizer.lemmatize(word.lower(),pos='v') for word in words if word not in set(stopwords.words('english'))]\n",
        "    sentences[i]=' '.join(words)# converting all the list of words into sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01zuArMd-RnQ",
        "outputId": "5a483dde-5c12-491b-995d-8dd42d1682e2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['three vision india .',\n",
              " '3000 year histori , peopl world come invad us , captur land , conquer mind .',\n",
              " 'alexand onward , greek , turk , mogul , portugu , british , french , dutch , come loot us , take .',\n",
              " 'yet do nation .',\n",
              " 'conquer anyon .',\n",
              " 'grab land , cultur , histori tri enforc way life .',\n",
              " 'whi ?',\n",
              " 'becaus respect freedom others.that first vision freedom .',\n",
              " 'believ india get first vision 1857 , start war independ .',\n",
              " 'freedom must protect nurtur build .',\n",
              " 'free , one respect us .',\n",
              " 'second vision india ’ develop .',\n",
              " 'fifti year develop nation .',\n",
              " 'time see develop nation .',\n",
              " 'among top 5 nation world term gdp .',\n",
              " '10 percent growth rate area .',\n",
              " 'poverti level fall .',\n",
              " 'achiev global recogni today .',\n",
              " 'yet lack self-confid see develop nation , self-r self-assur .',\n",
              " '’ incorrect ?',\n",
              " 'third vision .',\n",
              " 'india must stand world .',\n",
              " 'becaus believ unless india stand world , one respect us .',\n",
              " 'strength respect strength .',\n",
              " 'must strong militari power also econom power .',\n",
              " 'must go hand-in-hand .',\n",
              " 'good fortun work three great mind .',\n",
              " 'dr. vikram sarabhai dept .',\n",
              " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .',\n",
              " 'lucki work three close consid great opportun life .',\n",
              " 'see four mileston career']"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Parts OF speech Tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "l3B3YuJc-dG3"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "## Speech Of DR APJ Abdul Kalam\n",
        "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over\n",
        "               the world have come and invaded us, captured our lands, conquered our minds.\n",
        "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
        "               the French, the Dutch, all of them came and looted us, took over what was ours.\n",
        "               Yet we have not done this to any other nation. We have not conquered anyone.\n",
        "               We have not grabbed their land, their culture,\n",
        "               their history and tried to enforce our way of life on them.\n",
        "               Why? Because we respect the freedom of others.That is why my\n",
        "               first vision is that of freedom. I believe that India got its first vision of\n",
        "               this in 1857, when we started the War of Independence. It is this freedom that\n",
        "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
        "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
        "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
        "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
        "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
        "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
        "               I have a third vision. India must stand up to the world. Because I believe that unless India\n",
        "               stands up to the world, no one will respect us. Only strength respects strength. We must be\n",
        "               strong not only as a military power but also as an economic power. Both must go hand-in-hand.\n",
        "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of\n",
        "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
        "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.\n",
        "               I see four milestones in my career\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "yz6B4sn8_Kgd"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "sentences=nltk.sent_tokenize(paragraph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coUK2FLJ_NtQ",
        "outputId": "9e3dc98d-7962-41b6-c527-b588e8f5a8ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['I have three visions for India.',\n",
              " 'In 3000 years of our history, people from all over\\n               the world have come and invaded us, captured our lands, conquered our minds.',\n",
              " 'From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\\n               the French, the Dutch, all of them came and looted us, took over what was ours.',\n",
              " 'Yet we have not done this to any other nation.',\n",
              " 'We have not conquered anyone.',\n",
              " 'We have not grabbed their land, their culture,\\n               their history and tried to enforce our way of life on them.',\n",
              " 'Why?',\n",
              " 'Because we respect the freedom of others.That is why my\\n               first vision is that of freedom.',\n",
              " 'I believe that India got its first vision of\\n               this in 1857, when we started the War of Independence.',\n",
              " 'It is this freedom that\\n               we must protect and nurture and build on.',\n",
              " 'If we are not free, no one will respect us.',\n",
              " 'My second vision for India’s development.',\n",
              " 'For fifty years we have been a developing nation.',\n",
              " 'It is time we see ourselves as a developed nation.',\n",
              " 'We are among the top 5 nations of the world\\n               in terms of GDP.',\n",
              " 'We have a 10 percent growth rate in most areas.',\n",
              " 'Our poverty levels are falling.',\n",
              " 'Our achievements are being globally recognised today.',\n",
              " 'Yet we lack the self-confidence to\\n               see ourselves as a developed nation, self-reliant and self-assured.',\n",
              " 'Isn’t this incorrect?',\n",
              " 'I have a third vision.',\n",
              " 'India must stand up to the world.',\n",
              " 'Because I believe that unless India\\n               stands up to the world, no one will respect us.',\n",
              " 'Only strength respects strength.',\n",
              " 'We must be\\n               strong not only as a military power but also as an economic power.',\n",
              " 'Both must go hand-in-hand.',\n",
              " 'My good fortune was to have worked with three great minds.',\n",
              " 'Dr. Vikram Sarabhai of the Dept.',\n",
              " 'of\\n               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.',\n",
              " 'I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.',\n",
              " 'I see four milestones in my career']"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBiBhJEc_RI6",
        "outputId": "e12a7801-6349-47bb-d5ab-8d962367d42c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\Aditya\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will found out the Pos TAg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_JuP-Cz_Vtv",
        "outputId": "e464b585-a802-4854-8cda-dea5ddc3de48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('three', 'CD'), ('visions', 'NNS'), ('india', 'VBP'), ('.', '.')]\n",
            "[('3000', 'CD'), ('years', 'NNS'), ('history', 'NN'), (',', ','), ('people', 'NNS'), ('world', 'NN'), ('come', 'VBP'), ('invade', 'VBP'), ('us', 'PRP'), (',', ','), ('capture', 'NN'), ('land', 'NN'), (',', ','), ('conquer', 'NN'), ('mind', 'NN'), ('.', '.')]\n",
            "[('alexander', 'NN'), ('onwards', 'NNS'), (',', ','), ('greeks', 'NNS'), (',', ','), ('turks', 'NNS'), (',', ','), ('moguls', 'NNS'), (',', ','), ('portuguese', 'JJ'), (',', ','), ('british', 'JJ'), (',', ','), ('french', 'JJ'), (',', ','), ('dutch', 'VB'), (',', ','), ('come', 'VB'), ('loot', 'NN'), ('us', 'PRP'), (',', ','), ('take', 'VB'), ('.', '.')]\n",
            "[('yet', 'RB'), ('nation', 'NN'), ('.', '.')]\n",
            "[('conquer', 'NN'), ('anyone', 'NN'), ('.', '.')]\n",
            "[('grab', 'NN'), ('land', 'NN'), (',', ','), ('culture', 'NN'), (',', ','), ('history', 'NN'), ('try', 'NN'), ('enforce', 'NN'), ('way', 'NN'), ('life', 'NN'), ('.', '.')]\n",
            "[('?', '.')]\n",
            "[('respect', 'NN'), ('freedom', 'NN'), ('others.that', 'IN'), ('first', 'JJ'), ('vision', 'NN'), ('freedom', 'NN'), ('.', '.')]\n",
            "[('believe', 'VB'), ('india', 'NN'), ('get', 'NN'), ('first', 'JJ'), ('vision', 'NN'), ('1857', 'CD'), (',', ','), ('start', 'VBP'), ('war', 'NN'), ('independence', 'NN'), ('.', '.')]\n",
            "[('freedom', 'NN'), ('must', 'MD'), ('protect', 'VB'), ('nurture', 'NN'), ('build', 'NN'), ('.', '.')]\n",
            "[('free', 'JJ'), (',', ','), ('one', 'CD'), ('respect', 'NN'), ('us', 'PRP'), ('.', '.')]\n",
            "[('second', 'JJ'), ('vision', 'NN'), ('india', 'NN'), ('’', 'NNP'), ('development', 'NN'), ('.', '.')]\n",
            "[('fifty', 'JJ'), ('years', 'NNS'), ('develop', 'VB'), ('nation', 'NN'), ('.', '.')]\n",
            "[('time', 'NN'), ('see', 'VB'), ('develop', 'JJ'), ('nation', 'NN'), ('.', '.')]\n",
            "[('among', 'IN'), ('top', 'JJ'), ('5', 'CD'), ('nations', 'NNS'), ('world', 'NN'), ('term', 'NN'), ('gdp', 'NN'), ('.', '.')]\n",
            "[('10', 'CD'), ('percent', 'JJ'), ('growth', 'NN'), ('rate', 'NN'), ('areas', 'NNS'), ('.', '.')]\n",
            "[('poverty', 'NN'), ('level', 'NN'), ('fall', 'NN'), ('.', '.')]\n",
            "[('achievements', 'NNS'), ('globally', 'RB'), ('recognise', 'VBP'), ('today', 'NN'), ('.', '.')]\n",
            "[('yet', 'RB'), ('lack', 'JJ'), ('self-confidence', 'NN'), ('see', 'NN'), ('develop', 'VB'), ('nation', 'NN'), (',', ','), ('self-reliant', 'JJ'), ('self-assured', 'JJ'), ('.', '.')]\n",
            "[('’', 'NN'), ('incorrect', 'NN'), ('?', '.')]\n",
            "[('third', 'JJ'), ('vision', 'NN'), ('.', '.')]\n",
            "[('india', 'NN'), ('must', 'MD'), ('stand', 'VB'), ('world', 'NN'), ('.', '.')]\n",
            "[('believe', 'VB'), ('unless', 'IN'), ('india', 'JJ'), ('stand', 'NN'), ('world', 'NN'), (',', ','), ('one', 'CD'), ('respect', 'NN'), ('us', 'PRP'), ('.', '.')]\n",
            "[('strength', 'NN'), ('respect', 'NN'), ('strength', 'NN'), ('.', '.')]\n",
            "[('must', 'MD'), ('strong', 'JJ'), ('military', 'JJ'), ('power', 'NN'), ('also', 'RB'), ('economic', 'JJ'), ('power', 'NN'), ('.', '.')]\n",
            "[('must', 'MD'), ('go', 'VB'), ('hand-in-hand', 'NN'), ('.', '.')]\n",
            "[('good', 'JJ'), ('fortune', 'NN'), ('work', 'NN'), ('three', 'CD'), ('great', 'JJ'), ('mind', 'NN'), ('.', '.')]\n",
            "[('dr.', 'NN'), ('vikram', 'NN'), ('sarabhai', 'NN'), ('dept', 'NN'), ('.', '.')]\n",
            "[('space', 'NN'), (',', ','), ('professor', 'NN'), ('satish', 'JJ'), ('dhawan', 'NN'), (',', ','), ('succeed', 'VB'), ('dr.', 'JJ'), ('brahm', 'NN'), ('prakash', 'NN'), (',', ','), ('father', 'RB'), ('nuclear', 'JJ'), ('material', 'NN'), ('.', '.')]\n",
            "[('lucky', 'JJ'), ('work', 'NN'), ('three', 'CD'), ('closely', 'RB'), ('consider', 'VBP'), ('great', 'JJ'), ('opportunity', 'NN'), ('life', 'NN'), ('.', '.')]\n",
            "[('see', 'VB'), ('four', 'CD'), ('milestones', 'NNS'), ('career', 'NN')]\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(sentences)):\n",
        "    words=nltk.word_tokenize(sentences[i])\n",
        "    words=[word for word in words if word not in set(stopwords.words('english'))]\n",
        "    #sentences[i]=' '.join(words)# converting all the list of words into sentences\n",
        "    pos_tag=nltk.pos_tag(words)\n",
        "    print(pos_tag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7EZXeXq_aCW",
        "outputId": "26a614e9-1e1a-44c6-9cef-59c3a2b24492"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Taj', 'Mahal', 'is', 'a', 'beautiful', 'Monument']"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"Taj Mahal is a beautiful Monument\".split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adB0-kqD_4Ho",
        "outputId": "fe3d9016-50aa-48d3-b5db-38ac0d061f9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Taj', 'NNP'), ('Mahal', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('beautiful', 'JJ'), ('Monument', 'NN')]\n"
          ]
        }
      ],
      "source": [
        "print(nltk.pos_tag(\"Taj Mahal is a beautiful Monument\".split()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "                                        Named Entity Tag\n",
        "\n",
        "sentence=\"The Eiffel Tower was built from 1887 to 1889 by French engineer\n",
        "Gustave Eiffel, whose company specialized in building metal frameworks and\n",
        "structures.\"\n",
        "\"\"\"\n",
        "Person Eg: Krish C Naik\n",
        "Place Or Location Eg: India\n",
        "Date Eg: September,24-09-1989\n",
        "Time Eg: 4:30pm\n",
        "Money Eg: 1 million dollar\n",
        "Organization Eg: iNeuron Private Limited\n",
        "Percent Eg: 20%, twenty percent\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentence=\"The Eiffel Tower was built from 1887 to 1889 by French engineer Gustave Eiffel, whose company specialized in building metal frameworks and structures.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "words= nltk.word_tokenize(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "tag_elements=nltk.pos_tag(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     C:\\Users\\Aditya\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('maxent_ne_chunker')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to\n",
            "[nltk_data]     C:\\Users\\Aditya\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "nltk.ne_chunk(tag_elements).draw"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
